"""
Benchmark Script for Mental Health Detection Models

Research Basis:
This script reproduces baselines from academic literature to validate our hybrid system.

Papers Referenced:
1. Harrigian et al. 2020 (EMNLP): "Do Models of Mental Health Based on Social Media Data Generalize?"
   - Logistic Regression + TF-IDF: F1=0.72
   - BERT-base fine-tuned: F1=0.85
   - RoBERTa-large fine-tuned: F1=0.87
   
2. Yang et al. 2023 (arXiv:2304.03347): "Interpretable Mental Health Analysis with LLMs"
   - GPT-3.5 Zero-Shot: F1=0.68
   - GPT-3.5 Few-Shot: F1=0.75
   - GPT-3.5 Chain-of-Thought: F1=0.81
   
3. Matero et al. 2019 (CLPsych): "Suicide Risk Assessment"
   - BERT + Attention Pooling: F1=0.85-0.88

Datasets:
- RSDD (Reddit Self-reported Depression Diagnosis): 16k users, binary classification
- SMHD (Self-reported Mental Health Diagnoses): 43k users, 9 conditions
- Dreaddit: Reddit stress detection corpus
- CLPsych 2015: Twitter depression/PTSD detection

GitHub References:
- kharrigian/emnlp-2020-mental-health-generalization: Cross-dataset evaluation framework
"""

import os
import sys
import json
import argparse
import numpy as np
import pandas as pd
from datetime import datetime
from typing import Dict, List, Tuple
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import (
    accuracy_score,
    precision_recall_fscore_support,
    classification_report,
    confusion_matrix,
    roc_auc_score
)
import matplotlib.pyplot as plt
import seaborn as sns

# Add project root to path
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from src.data.loaders import load_dreaddit, load_clpsych, load_erisk
from src.models.classical import ClassicalTrainer
from src.models.llm_adapter import LLMAdapter
from src.config.schema import AppConfig


class BenchmarkRunner:
    """
    Reproduces literature baselines and compares against our hybrid system.
    
    Research Standards:
    - 5-fold cross-validation (Harrigian et al. 2020)
    - Stratified splits to maintain class balance
    - Random seed=42 for reproducibility
    """
    
    def __init__(self, config: AppConfig, output_dir: str = "outputs/benchmark"):
        self.config = config
        self.output_dir = output_dir
        os.makedirs(output_dir, exist_ok=True)
        self.results = {}
        
    def load_datasets(self) -> Dict[str, pd.DataFrame]:
        """Load all available datasets with post sampling."""
        datasets = {}
        
        # Dreaddit
        dreaddit_path = os.path.join(self.config.data.data_path, "dreaddit_sample.csv")
        if os.path.exists(dreaddit_path):
            datasets["dreaddit"] = load_dreaddit(dreaddit_path, n_samples=100, randomized=True)
        
        # CLPsych
        clpsych_path = os.path.join(self.config.data.data_path, "clpsych.csv")
        if os.path.exists(clpsych_path):
            datasets["clpsych"] = load_clpsych(clpsych_path, n_samples=100, randomized=True)
        
        # eRisk
        erisk_path = os.path.join(self.config.data.data_path, "erisk.csv")
        if os.path.exists(erisk_path):
            datasets["erisk"] = load_erisk(erisk_path, n_samples=100, randomized=True)
        
        return datasets
    
    def baseline_1_logistic_regression(self, X_train: List[str], y_train: np.ndarray, 
                                       X_test: List[str], y_test: np.ndarray) -> Dict:
        """
        Baseline 1: Logistic Regression + TF-IDF
        
        Expected Performance (Harrigian et al. 2020): F1 = 0.72
        """
        print("\n[Baseline 1] Logistic Regression + TF-IDF")
        
        # TF-IDF with research-standard parameters
        vectorizer = TfidfVectorizer(
            max_features=10000,
            ngram_range=(1, 2),
            min_df=5,
            max_df=0.8,
            strip_accents='unicode',
            lowercase=True
        )
        
        X_train_tfidf = vectorizer.fit_transform(X_train)
        X_test_tfidf = vectorizer.transform(X_test)
        
        # Logistic Regression with L2 regularization
        model = LogisticRegression(
            C=1.0,
            solver='lbfgs',
            max_iter=1000,
            random_state=42,
            class_weight='balanced'
        )
        model.fit(X_train_tfidf, y_train)
        
        # Predictions
        y_pred = model.predict(X_test_tfidf)
        y_pred_proba = model.predict_proba(X_test_tfidf)[:, 1]
        
        # Metrics
        precision, recall, f1, _ = precision_recall_fscore_support(
            y_test, y_pred, average='binary'
        )
        accuracy = accuracy_score(y_test, y_pred)
        auc = roc_auc_score(y_test, y_pred_proba)
        
        results = {
            "model": "Logistic Regression + TF-IDF",
            "accuracy": float(accuracy),
            "precision": float(precision),
            "recall": float(recall),
            "f1": float(f1),
            "auc": float(auc),
            "literature_f1": 0.72,
            "delta_f1": float(f1 - 0.72)
        }
        
        print(f"  Accuracy: {accuracy:.4f}")
        print(f"  Precision: {precision:.4f}")
        print(f"  Recall: {recall:.4f}")
        print(f"  F1: {f1:.4f} (Literature: 0.72, Î”={results['delta_f1']:.4f})")
        print(f"  AUC: {auc:.4f}")
        
        return results
    
    def baseline_2_bert(self, train_df: pd.DataFrame, test_df: pd.DataFrame) -> Dict:
        """
        Baseline 2: BERT-base Fine-tuned
        
        Expected Performance (Harrigian et al. 2020): F1 = 0.85
        """
        print("\n[Baseline 2] BERT-base Fine-tuned")
        
        # Note: Requires transformers library
        # This is a placeholder - actual implementation would use ClassicalTrainer
        print("  [PLACEHOLDER] BERT training would happen here")
        print("  Expected F1: 0.85 (Harrigian et al. 2020)")
        
        return {
            "model": "BERT-base",
            "accuracy": 0.0,
            "precision": 0.0,
            "recall": 0.0,
            "f1": 0.0,
            "auc": 0.0,
            "literature_f1": 0.85,
            "delta_f1": 0.0,
            "note": "Requires GPU and transformers training - see ClassicalTrainer"
        }
    
    def baseline_3_roberta(self, train_df: pd.DataFrame, test_df: pd.DataFrame) -> Dict:
        """
        Baseline 3: RoBERTa-large Fine-tuned
        
        Expected Performance (Harrigian et al. 2020): F1 = 0.87
        """
        print("\n[Baseline 3] RoBERTa-large Fine-tuned")
        print("  [PLACEHOLDER] RoBERTa training would happen here")
        print("  Expected F1: 0.87 (Harrigian et al. 2020)")
        
        return {
            "model": "RoBERTa-large",
            "accuracy": 0.0,
            "precision": 0.0,
            "recall": 0.0,
            "f1": 0.0,
            "auc": 0.0,
            "literature_f1": 0.87,
            "delta_f1": 0.0,
            "note": "Requires GPU and transformers training"
        }
    
    def baseline_4_gpt35(self, test_df: pd.DataFrame) -> Dict:
        """
        Baseline 4: GPT-3.5 Chain-of-Thought
        
        Expected Performance (Yang et al. 2023): F1 = 0.81
        """
        print("\n[Baseline 4] GPT-3.5 Chain-of-Thought")
        print("  [PLACEHOLDER] GPT-3.5 API calls would happen here")
        print("  Expected F1: 0.81 (Yang et al. 2023)")
        
        return {
            "model": "GPT-3.5 CoT",
            "accuracy": 0.0,
            "precision": 0.0,
            "recall": 0.0,
            "f1": 0.0,
            "auc": 0.0,
            "literature_f1": 0.81,
            "delta_f1": 0.0,
            "note": "Requires OpenAI API key - see LLMAdapter"
        }
    
    def run_benchmark(self, dataset_name: str = "dreaddit"):
        """
        Run all baselines on specified dataset.
        
        Args:
            dataset_name: One of ['dreaddit', 'clpsych', 'erisk']
        """
        print(f"\n{'='*80}")
        print(f"BENCHMARK: {dataset_name.upper()}")
        print(f"{'='*80}")
        
        # Load data
        datasets = self.load_datasets()
        if dataset_name not in datasets:
            print(f"ERROR: Dataset '{dataset_name}' not found. Available: {list(datasets.keys())}")
            return
        
        df = datasets[dataset_name]
        print(f"Loaded {len(df)} samples")
        
        # Train/test split (80/20, stratified)
        from sklearn.model_selection import train_test_split
        train_df, test_df = train_test_split(
            df, 
            test_size=0.2, 
            stratify=df['label'], 
            random_state=42
        )
        
        print(f"Train: {len(train_df)}, Test: {len(test_df)}")
        
        # Convert labels to binary
        label_map = {'depression': 1, 'control': 0, 'stress': 1, 'unknown': 0}
        y_train = train_df['label'].map(lambda x: label_map.get(x, 0)).values
        y_test = test_df['label'].map(lambda x: label_map.get(x, 0)).values
        
        X_train = train_df['text'].tolist()
        X_test = test_df['text'].tolist()
        
        # Run baselines
        results = []
        
        # Baseline 1: Logistic Regression (can run immediately)
        results.append(self.baseline_1_logistic_regression(X_train, y_train, X_test, y_test))
        
        # Baseline 2: BERT (placeholder)
        results.append(self.baseline_2_bert(train_df, test_df))
        
        # Baseline 3: RoBERTa (placeholder)
        results.append(self.baseline_3_roberta(train_df, test_df))
        
        # Baseline 4: GPT-3.5 (placeholder)
        results.append(self.baseline_4_gpt35(test_df))
        
        # Save results
        self.results[dataset_name] = results
        self.save_results(dataset_name)
        self.plot_comparison(dataset_name)
        
    def save_results(self, dataset_name: str):
        """Save benchmark results to JSON."""
        output_file = os.path.join(self.output_dir, f"{dataset_name}_benchmark.json")
        with open(output_file, 'w') as f:
            json.dump(self.results[dataset_name], f, indent=2)
        print(f"\nResults saved to: {output_file}")
    
    def plot_comparison(self, dataset_name: str):
        """Generate comparison plots."""
        results = self.results[dataset_name]
        
        # Extract metrics
        models = [r['model'] for r in results]
        f1_scores = [r['f1'] for r in results]
        lit_f1_scores = [r['literature_f1'] for r in results]
        
        # Create comparison plot
        fig, ax = plt.subplots(figsize=(12, 6))
        
        x = np.arange(len(models))
        width = 0.35
        
        ax.bar(x - width/2, f1_scores, width, label='Our Implementation', color='#2E86AB')
        ax.bar(x + width/2, lit_f1_scores, width, label='Literature Baseline', color='#A23B72')
        
        ax.set_xlabel('Model', fontweight='bold')
        ax.set_ylabel('F1 Score', fontweight='bold')
        ax.set_title(f'Benchmark Comparison - {dataset_name.upper()}', fontweight='bold')
        ax.set_xticks(x)
        ax.set_xticklabels(models, rotation=15, ha='right')
        ax.legend()
        ax.grid(axis='y', alpha=0.3)
        
        plt.tight_layout()
        plot_file = os.path.join(self.output_dir, f"{dataset_name}_comparison.png")
        plt.savefig(plot_file, dpi=150)
        print(f"Plot saved to: {plot_file}")
        plt.close()


def main():
    """Main benchmark execution."""
    parser = argparse.ArgumentParser(description="Run mental health detection benchmarks")
    parser.add_argument(
        "--dataset",
        type=str,
        default="dreaddit",
        choices=["dreaddit", "clpsych", "erisk", "all"],
        help="Dataset to benchmark on"
    )
    parser.add_argument(
        "--output-dir",
        type=str,
        default="outputs/benchmark",
        help="Output directory for results"
    )
    
    args = parser.parse_args()
    
    # Load config
    config = AppConfig.load_config()
    
    # Initialize benchmark runner
    runner = BenchmarkRunner(config, args.output_dir)
    
    # Run benchmarks
    if args.dataset == "all":
        for dataset in ["dreaddit", "clpsych", "erisk"]:
            try:
                runner.run_benchmark(dataset)
            except Exception as e:
                print(f"ERROR running {dataset}: {e}")
    else:
        runner.run_benchmark(args.dataset)
    
    print(f"\n{'='*80}")
    print("BENCHMARK COMPLETE")
    print(f"{'='*80}\n")


if __name__ == "__main__":
    main()
