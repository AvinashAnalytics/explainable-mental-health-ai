# üöÄ Groq API Setup & Model Comparison Guide

## What is Groq?

**Groq** provides **FAST** inference for open-source LLMs:
- ‚ö° **10-100x faster** than OpenAI
- üÜì **Free tier** available (14,400 requests/day)
- üîì **Open models**: Llama-3, Mixtral, Gemma
- üí∞ **Cheap**: $0.05-$0.27 per 1M tokens (vs OpenAI $0.50-$15)

---

## Step 1: Get Groq API Key (FREE)

1. Visit: https://console.groq.com/keys
2. Sign up (free, no credit card)
3. Create API key
4. Copy the key (starts with `gsk_...`)

---

## Step 2: Set API Key

### Windows PowerShell:
```powershell
$env:GROQ_API_KEY="gsk_your_key_here"
```

### Linux/Mac:
```bash
export GROQ_API_KEY="gsk_your_key_here"
```

### Or add to .env file:
```bash
echo "GROQ_API_KEY=gsk_your_key_here" >> .env
```

---

## Step 3: Install Groq SDK

```bash
pip install groq
```

---

## Step 4: Test Single Model (Quick)

### Test Llama 3.1 70B (Recommended):
```bash
python compare_groq_models.py --model llama-3.1-70b
```

### Test Mixtral 8x7B:
```bash
python compare_groq_models.py --model mixtral-8x7b
```

### Test with Chain-of-Thought:
```bash
python compare_groq_models.py --model llama-3.1-70b --method cot
```

---

## Step 5: Compare All Models (10 samples)

```bash
python compare_groq_models.py --compare-all
```

This will test **7 models** on 10 samples and rank them by F1 score.

---

## Available Groq Models

| Model | Size | Speed | Quality | Context | Best For |
|-------|------|-------|---------|---------|----------|
| **llama-3.1-70b** | 70B | Medium | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | 128K | **Best accuracy** |
| **llama-3.1-8b** | 8B | Fast | ‚≠ê‚≠ê‚≠ê‚≠ê | 128K | **Fast & good** |
| llama-3-70b | 70B | Medium | ‚≠ê‚≠ê‚≠ê‚≠ê | 8K | General purpose |
| llama-3-8b | 8B | Fast | ‚≠ê‚≠ê‚≠ê | 8K | Quick inference |
| **mixtral-8x7b** | 47B | Medium | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | 32K | **High quality** |
| gemma-7b | 7B | Fast | ‚≠ê‚≠ê‚≠ê | 8K | Efficient |
| gemma2-9b | 9B | Fast | ‚≠ê‚≠ê‚≠ê‚≠ê | 8K | Latest Gemma |

**Recommended**: `llama-3.1-70b` (best accuracy) or `mixtral-8x7b` (high quality)

---

## Step 6: Use Best Model in Your Project

After finding the best model, update your config:

### Edit `configs/config.yaml`:
```yaml
model:
  llm:
    provider: "groq"           # Changed from 'openai'
    model_name: "llama-3.1-70b"  # Best model
    temperature: 0.2
    max_tokens: 512
```

### Or use directly in code:
```python
from src.models.llm_adapter import MentalHealthLLM

# Use Groq with Llama 3.1 70B
llm = MentalHealthLLM(
    provider='groq',
    model='llama-3.1-70b'
)

# Analyze text
result = llm.analyze(
    "I feel hopeless and empty. Nothing brings me joy anymore.",
    method='zero_shot'
)

print(result['depression_likelihood'])
print(result['dsm_symptoms'])
print(result['explanation'])
```

---

## Prompting Methods

### 1. **zero_shot** (Default - Fast)
- Simple instruction
- No examples
- Best for: Speed

```bash
python compare_groq_models.py --model llama-3.1-70b --method zero_shot
```

### 2. **few_shot** (Better Accuracy)
- Includes 1-5 examples
- Better consistency
- Best for: Accuracy

```bash
python compare_groq_models.py --model llama-3.1-70b --method few_shot
```

### 3. **cot** (Chain-of-Thought - Best Reasoning)
- Step-by-step reasoning
- Best explanations
- Best for: Interpretability

```bash
python compare_groq_models.py --model llama-3.1-70b --method cot
```

### 4. **emotion_cot** (Depression-Specific)
- Emotion-aware reasoning
- DSM-5 focused
- Best for: Clinical validity

```bash
python compare_groq_models.py --model llama-3.1-70b --method emotion_cot
```

---

## Full Comparison (All Models + All Methods)

```bash
# Test all models with zero-shot
python compare_groq_models.py --compare-all --method zero_shot

# Test all models with CoT
python compare_groq_models.py --compare-all --method cot

# Test all models with few-shot
python compare_groq_models.py --compare-all --method few_shot

# Test all models with emotion CoT
python compare_groq_models.py --compare-all --method emotion_cot
```

---

## Expected Results

Based on research (Yang et al. 2023), expected performance:

| Model | F1 Score | Accuracy | Speed |
|-------|----------|----------|-------|
| **Llama 3.1 70B + CoT** | **0.87-0.91** | 88-92% | 2-3s |
| Mixtral 8x7B + CoT | 0.85-0.89 | 86-90% | 2-3s |
| Llama 3.1 8B + Zero-Shot | 0.82-0.86 | 83-87% | 0.5-1s |
| GPT-4 (OpenAI) | 0.87-0.90 | 87-90% | 5-10s |

**Groq is 5-10x faster than OpenAI!** ‚ö°

---

## Batch Testing on Real Data

```python
# Test on your 1000 samples
python -c "
import os
os.environ['GROQ_API_KEY'] = 'gsk_your_key'

from src.models.llm_adapter import MentalHealthLLM
import pandas as pd
from sklearn.metrics import accuracy_score, f1_score

# Load data
df = pd.read_csv('data/dreaddit-train.csv')
samples = df.head(100)  # Test on 100 samples

# Initialize Groq
llm = MentalHealthLLM(provider='groq', model='llama-3.1-70b')

# Predict
predictions = []
for text in samples['text']:
    result = llm.analyze(text, method='zero_shot')
    likelihood = result.get('depression_likelihood', 'Low')
    pred = 1 if likelihood in ['High', 'Moderate'] else 0
    predictions.append(pred)

# Metrics
acc = accuracy_score(samples['label'], predictions)
f1 = f1_score(samples['label'], predictions)

print(f'Accuracy: {acc:.2%}')
print(f'F1 Score: {f1:.2%}')
"
```

---

## Cost Comparison

### Groq (Fast & Cheap):
- Llama 3.1 70B: **$0.05/1M tokens** input, $0.27/1M output
- Llama 3.1 8B: **$0.05/1M tokens** input, $0.08/1M output
- Mixtral 8x7B: **$0.24/1M tokens** input, $0.24/1M output
- **Free tier**: 14,400 requests/day

### OpenAI (Slower & Expensive):
- GPT-4o: $2.50/1M input, $10/1M output
- GPT-4o-mini: $0.15/1M input, $0.60/1M output
- No free tier

**For 1000 samples (~200K tokens):**
- Groq: **$0.01** (FREE tier)
- OpenAI: **$0.50-$2.00**

---

## Integration with Existing Code

Your project now supports 3 providers:

### 1. OpenAI (Existing):
```python
llm = MentalHealthLLM(provider='openai', model='gpt-4o-mini')
```

### 2. Groq (NEW - Recommended):
```python
llm = MentalHealthLLM(provider='groq', model='llama-3.1-70b')
```

### 3. HuggingFace (Local):
```python
llm = MentalHealthLLM(provider='huggingface', model='meta-llama/Llama-2-7b')
```

---

## Troubleshooting

### Error: "groq package not installed"
```bash
pip install groq
```

### Error: "GROQ_API_KEY not set"
```bash
$env:GROQ_API_KEY="gsk_your_key"
```

### Error: "Rate limit exceeded"
- Free tier: 30 requests/minute, 14,400/day
- Add `time.sleep(2)` between requests
- Or upgrade to paid tier

### Slow inference?
- Use smaller model: `llama-3.1-8b` (10x faster)
- Use `zero_shot` instead of `cot`
- Enable batch processing

---

## Next Steps

1. ‚úÖ Get free Groq API key
2. ‚úÖ Run quick test: `python compare_groq_models.py --model llama-3.1-70b`
3. ‚úÖ Compare all models: `python compare_groq_models.py --compare-all`
4. ‚úÖ Use best model in your project
5. ‚úÖ Test on your 1000 real samples

---

## Resources

- **Groq Console**: https://console.groq.com
- **API Docs**: https://console.groq.com/docs
- **Model Specs**: https://wow.groq.com/models/
- **Rate Limits**: https://console.groq.com/settings/limits

---

**Ready to test? Run this:**

```bash
# Set API key
$env:GROQ_API_KEY="gsk_your_key_here"

# Quick test (3 samples)
python compare_groq_models.py --model llama-3.1-70b

# Full comparison (10 samples, 7 models)
python compare_groq_models.py --compare-all

# See results
cat groq_model_comparison_zero_shot.json
```

üéâ **Enjoy 10x faster LLM inference!** ‚ö°
