# üöÄ Quick Start Commands

## ‚úÖ Step 1: Validate Everything Works

```bash
# Run all tests (should all pass)
python test_phase1.py           # 4/4 tests
python test_new_features.py     # 6/6 tests  
python test_model_comparison.py # 7/7 tests
```

**Expected**: All tests passing (100% success rate)

---

## üìä Step 2: Get Dataset

### Option A: Use Existing Sample (Fastest)
```bash
# Already have data/dreaddit_sample.csv (1000 samples)
# Nothing to do - ready to train!
```

### Option B: Create Mock Dataset
```bash
python download_datasets.py
# Follow prompts to create mock dataset
```

### Option C: Download Real Datasets
```bash
python download_datasets.py --all
# Provides instructions for RSDD, SMHD, CLPsych, eRisk
```

---

## üî• Step 3: Train Your First Model

### Train RoBERTa (Best Accuracy)
```bash
python train_depression_classifier.py \
  --model roberta-base \
  --data data/dreaddit_sample.csv \
  --epochs 3 \
  --batch-size 16
```
**Requirements**: 8-10GB GPU memory  
**Time**: ~15-30 minutes  
**Expected F1**: 0.84-0.89

### Train DistilBERT (Fastest)
```bash
python train_depression_classifier.py \
  --model distilbert-base-uncased \
  --data data/dreaddit_sample.csv \
  --epochs 3 \
  --batch-size 16
```
**Requirements**: 4GB GPU memory  
**Time**: ~10-20 minutes  
**Expected F1**: 0.79-0.84

### Train BERT (Baseline)
```bash
python train_depression_classifier.py \
  --model bert-base-uncased \
  --data data/dreaddit_sample.csv \
  --epochs 3 \
  --batch-size 16
```
**Requirements**: 6-8GB GPU memory  
**Time**: ~12-25 minutes  
**Expected F1**: 0.81-0.87

---

## üéØ Step 4: Make Predictions

### Single Text Prediction
```bash
python predict_depression.py \
  --model models/trained/roberta_20251125_120000 \
  --text "I feel hopeless and can't sleep. Nothing matters anymore."
```

**Output**:
```json
{
  "text": "I feel hopeless and can't sleep...",
  "prediction": "depression",
  "confidence": 0.89,
  "attention_weights": {
    "hopeless": 0.45,
    "can't": 0.32,
    "sleep": 0.28
  },
  "llm_explanation": "The text shows signs of depressed mood...",
  "dsm5_symptoms": ["A1_depressed_mood", "A4_sleep_disturbance"],
  "phq9_score": 12
}
```

### Batch CSV Prediction
```bash
python predict_depression.py \
  --model models/trained/roberta_20251125_120000 \
  --csv data/test_samples.csv \
  --output results.json
```

**Input CSV Format**:
```csv
text
"I feel sad and tired all the time"
"Had a great day with friends"
"Can't concentrate on anything"
```

---

## üìä Step 5: Compare Models

### Compare All Trained Models
```bash
python compare_models.py \
  --models models/trained/roberta_* models/trained/bert_* models/trained/distilbert_* \
  --test-data data/dreaddit_sample.csv
```

**Output**: `comparison_report_20251125.json`
```json
{
  "models": {
    "roberta-base": {"accuracy": 0.87, "f1": 0.86},
    "bert-base": {"accuracy": 0.85, "f1": 0.84},
    "distilbert-base": {"accuracy": 0.82, "f1": 0.81}
  },
  "best_model": "roberta-base",
  "recommendation": "Use RoBERTa for highest accuracy"
}
```

---

## üåê Step 6: Run Web Interface

```bash
streamlit run src/app/app.py
```

**Access**: http://localhost:8501

**Features**:
- Upload CSV or enter text
- Real-time predictions
- Attention visualizations
- DSM-5/PHQ-9 scoring
- LLM explanations (if API keys set)

---

## üîë Step 7: Add LLM Explanations (Optional)

### Setup Groq API (Recommended - Free & Fast)
```bash
# Windows PowerShell
$env:GROQ_API_KEY = "gsk_your_groq_api_key_here"

# Linux/Mac
export GROQ_API_KEY="gsk_your_groq_api_key_here"
```

**Get Key**: https://console.groq.com/keys

### Setup OpenAI API (Alternative)
```bash
# Windows PowerShell
$env:OPENAI_API_KEY = "sk-your_openai_api_key_here"

# Linux/Mac
export OPENAI_API_KEY="sk-your_openai_api_key_here"
```

**Get Key**: https://platform.openai.com/api-keys

### Test LLM Integration
```bash
python predict_depression.py \
  --model models/trained/roberta_* \
  --text "I feel worthless" \
  --llm-provider groq \
  --llm-model llama-3.1-70b-versatile
```

---

## üìì Step 8: Use Jupyter Notebook (Research)

```bash
# Install Jupyter (if not installed)
pip install jupyter

# Launch notebook
jupyter notebook notebooks/fine_tune_depression_detection.ipynb
```

**Notebook Features**:
- Interactive fine-tuning
- Step-by-step explanations
- Visualization of results
- Experimentation with hyperparameters
- Model comparison

---

## üîß Troubleshooting

### LIME Not Working
```bash
pip install lime
```

### SHAP Not Working
```bash
pip install shap
```

### GPU Not Detected
```bash
# Check CUDA
python -c "import torch; print(torch.cuda.is_available())"

# If False, install CUDA or use CPU
python train_depression_classifier.py --model roberta-base --device cpu
```

### Out of Memory
```bash
# Reduce batch size
python train_depression_classifier.py --model roberta-base --batch-size 8

# Or use smaller model
python train_depression_classifier.py --model distilbert-base-uncased
```

### API Errors
```bash
# Check if key is set
echo $env:GROQ_API_KEY  # Windows
echo $GROQ_API_KEY      # Linux/Mac

# Test API
python -c "import os; from groq import Groq; client = Groq(api_key=os.getenv('GROQ_API_KEY')); print('API Working!')"
```

---

## üìö Next Steps

### For Learning
1. Read `README.md` - Complete overview
2. Read `QUICK_START.md` - Detailed guide
3. Run tests to understand features
4. Experiment with Jupyter notebook

### For Research
1. Fine-tune on larger dataset (3K-8K samples)
2. Compare multiple models
3. Analyze faithfulness metrics
4. Generate paper figures

### For Production
1. Train on large dataset (20K+ samples)
2. Calibrate confidence scores
3. Set up API endpoints
4. Implement monitoring

---

## üéì Example Workflow

```bash
# 1. Validate setup
python test_phase1.py

# 2. Train model
python train_depression_classifier.py --model roberta-base --data data/dreaddit_sample.csv

# 3. Test single prediction
python predict_depression.py --model models/trained/roberta_* --text "I feel sad"

# 4. Batch predictions
python predict_depression.py --model models/trained/roberta_* --csv data/test.csv --output results.json

# 5. Compare models (if multiple trained)
python compare_models.py --models models/trained/* --test-data data/dreaddit_sample.csv

# 6. Launch web interface
streamlit run src/app/app.py
```

---

## ‚úÖ Checklist

- [ ] Install dependencies (`pip install -r requirements.txt`)
- [ ] Run tests (all should pass)
- [ ] Have dataset ready (use `dreaddit_sample.csv` or download)
- [ ] Train first model (~20 minutes)
- [ ] Test predictions
- [ ] (Optional) Set API keys for LLM explanations
- [ ] (Optional) Launch web interface
- [ ] (Optional) Compare multiple models

---

## üìû Help

**Documentation**:
- `README.md` - Overview
- `PROJECT_STRUCTURE.md` - Structure
- `TRAINING_GUIDE.md` - Training details
- `TESTING_GUIDE.md` - Testing details

**Common Questions**:
- Which model to use? ‚Üí RoBERTa (best accuracy), DistilBERT (fastest)
- How much data needed? ‚Üí Min 500, Good 3K-8K, Best 20K+
- How long to train? ‚Üí 10-30 minutes depending on model/GPU
- Need API keys? ‚Üí No (for basic), Yes (for LLM explanations)

---

**Last Updated**: November 25, 2025  
**Status**: Production Ready ‚úÖ  
**Success Rate**: 100% (17/17 tests passing)
