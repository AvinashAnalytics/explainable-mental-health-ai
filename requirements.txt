# ====================================================================
# DEPRESSION DETECTION WITH EXPLAINABLE AI - REQUIREMENTS
# ====================================================================
# CS 772 Final Project - Mental Health Classification System
# Python 3.9+ required
# ====================================================================

# ====================================================================
# CORE MACHINE LEARNING & DEEP LEARNING FRAMEWORKS
# ====================================================================

torch>=2.0.0
# PyTorch: Core deep learning framework for transformer models
# Used for: Training RoBERTa/BERT models, neural network operations
# GPU acceleration: Requires CUDA 11.8+ for GPU support (optional)

transformers>=4.30.0
# HuggingFace Transformers: Pre-trained language model library
# Used for: RoBERTa-Base, BERT, DistilBERT, MentalBERT implementations
# Includes: Model loading, tokenization, fine-tuning capabilities

scikit-learn>=1.3.0
# Scikit-learn: Classical machine learning library
# Used for: Baseline models (SVM, Logistic Regression, Random Forest)
# Also provides: TF-IDF vectorization, metrics, cross-validation

# ====================================================================
# EXPLAINABLE AI (XAI) FRAMEWORKS
# ====================================================================

captum>=0.6.0
# Captum: Model interpretability library for PyTorch
# Used for: Integrated Gradients implementation (token attribution)
# Provides: Layer-wise relevance propagation, feature importance

shap>=0.42.0
# SHAP: SHapley Additive exPlanations
# Used for: Model-agnostic feature importance explanations
# Provides: TreeExplainer, KernelExplainer, force plots

lime>=0.2.0.1
# LIME: Local Interpretable Model-agnostic Explanations
# Used for: Local linear approximations of model predictions
# Provides: Text explainer, tabular explainer

# ====================================================================
# NATURAL LANGUAGE PROCESSING (NLP)
# ====================================================================

spacy>=3.5.0
# spaCy: Industrial-strength NLP library
# Used for: Text preprocessing, tokenization, NER, POS tagging
# Note: Requires model download: python -m spacy download en_core_web_sm

nltk>=3.8.1
# NLTK: Natural Language Toolkit
# Used for: Stopword removal, tokenization, text normalization
# Note: Requires data download: nltk.download('stopwords')

tokenizers>=0.13.0
# Fast tokenizers for transformer models
# Used for: High-performance tokenization with HuggingFace models
# Provides: Byte-Pair Encoding (BPE), WordPiece tokenization

textstat>=0.7.3
# Text readability metrics
# Used for: Flesch-Kincaid, SMOG, Coleman-Liau readability scores
# Helps: Analyze linguistic complexity of mental health texts

emoji>=2.8.0
# Emoji extraction and analysis
# Used for: Detect emotional indicators in social media text
# Provides: Emoji sentiment analysis, demojization

# ====================================================================
# LARGE LANGUAGE MODEL (LLM) INTEGRATION
# ====================================================================

openai>=1.0.0
# OpenAI API client
# Used for: GPT-4o clinical explanations and reasoning
# Requires: OPENAI_API_KEY environment variable
# Cost: ~$0.0003 per prediction

groq>=0.4.0
# Groq API client (alternative LLM provider)
# Used for: Fast LLM inference with Llama models
# Requires: GROQ_API_KEY environment variable

# ====================================================================
# DATA PROCESSING & MANIPULATION
# ====================================================================

pandas>=2.0.0
# Pandas: Data manipulation and analysis
# Used for: DataFrame operations, CSV loading, data merging
# Handles: Dreaddit, CLPsych, eRisk datasets

numpy>=1.24.0
# NumPy: Numerical computing library
# Used for: Array operations, mathematical functions
# Required by: PyTorch, scikit-learn, pandas

datasets>=2.14.0
# HuggingFace Datasets: Dataset loading and processing
# Used for: Efficient data loading, caching, preprocessing
# Provides: Memory-mapped datasets, streaming support

# ====================================================================
# WEB APPLICATION FRAMEWORK
# ====================================================================

streamlit>=1.24.0
# Streamlit: Interactive web application framework
# Used for: Main application UI (src/app/app.py - 770 lines)
# Features: Single/batch prediction, explainability dashboard, crisis detection
# Deployment: Runs on port 8501 by default

# ====================================================================
# DATA VISUALIZATION
# ====================================================================

matplotlib>=3.7.0
# Matplotlib: Comprehensive plotting library
# Used for: Token attribution visualizations, confusion matrices
# Provides: Static plots, heatmaps, bar charts

seaborn>=0.12.0
# Seaborn: Statistical data visualization
# Used for: Enhanced matplotlib plots, correlation matrices
# Provides: Beautiful default themes, statistical plots

plotly>=5.14.0
# Plotly: Interactive visualization library
# Used for: Interactive explainability dashboards
# Provides: Hover tooltips, zoom, pan capabilities

# ====================================================================
# CONFIGURATION & UTILITIES
# ====================================================================

pyyaml>=6.0.1
# PyYAML: YAML parser and emitter
# Used for: Configuration file parsing (config.yaml)
# Handles: Model hyperparameters, training settings

pydantic>=2.0.0
# Pydantic: Data validation using Python type hints
# Used for: Configuration validation, API request/response models
# Provides: Type checking, automatic validation

python-dotenv>=1.0.0
# Python-dotenv: Environment variable management
# Used for: Loading API keys from .env file
# Security: Keeps secrets out of code (OPENAI_API_KEY, GROQ_API_KEY)

tqdm>=4.65.0
# tqdm: Progress bar library
# Used for: Training progress, data loading progress
# Provides: ETA, iteration speed, customizable progress bars

requests>=2.31.0
# Requests: HTTP library for API calls
# Used for: External API requests (if needed)
# Provides: Session management, retries

# ====================================================================
# DEVELOPMENT & TESTING
# ====================================================================

pytest>=7.4.0
# Pytest: Testing framework
# Used for: Unit tests, integration tests
# Provides: Fixtures, parameterized tests, coverage

black>=23.7.0
# Black: Code formatter
# Used for: Automatic code formatting (PEP 8 compliant)
# Usage: black src/ --line-length 100

flake8>=6.0.0
# Flake8: Linting tool
# Used for: Code quality checks, style violations
# Checks: PEP 8 compliance, unused imports, complexity

mypy>=1.4.0
# Mypy: Static type checker
# Used for: Type hint validation
# Provides: Type safety, early bug detection

ipython>=8.14.0
# IPython: Enhanced interactive Python shell
# Used for: Debugging, interactive development
# Provides: Tab completion, magic commands, rich output

# ====================================================================
# PRODUCTION & DEPLOYMENT (OPTIONAL)
# ====================================================================

gunicorn>=21.2.0
# Gunicorn: WSGI HTTP server (Linux/macOS only)
# Used for: Production deployment of Streamlit app
# Note: Use waitress on Windows

docker>=6.1.0
# Docker SDK: Containerization support
# Used for: Building Docker images, container management
# Requires: Docker Desktop installed

# ====================================================================
# ADDITIONAL DEPENDENCIES
# ====================================================================

scipy>=1.10.0
# SciPy: Scientific computing library
# Used for: Statistical functions, optimization
# Required by: scikit-learn, SHAP

joblib>=1.3.0
# Joblib: Lightweight pipelining and caching
# Used for: Model serialization, parallel processing
# Provides: Efficient model saving/loading

rouge-score>=0.1.2
# ROUGE: Evaluation metric for text generation
# Used for: Evaluating LLM explanation quality
# Provides: ROUGE-1, ROUGE-2, ROUGE-L scores

# ====================================================================
# INSTALLATION INSTRUCTIONS
# ====================================================================
#
# 1. Create virtual environment:
#    python -m venv .venv
#
# 2. Activate virtual environment:
#    Windows:   .venv\Scripts\activate
#    Linux/Mac: source .venv/bin/activate
#
# 3. Install all packages:
#    pip install -r requirements.txt
#
# 4. Install spaCy language model:
#    python -m spacy download en_core_web_sm
#
# 5. Download NLTK data:
#    python -c "import nltk; nltk.download('stopwords'); nltk.download('punkt')"
#
# 6. Set up environment variables (.env file):
#    OPENAI_API_KEY=your_openai_api_key_here
#    GROQ_API_KEY=your_groq_api_key_here
#
# 7. Verify installation:
#    python -c "import torch; import transformers; print('Success!')"
#
# ====================================================================
# SYSTEM REQUIREMENTS
# ====================================================================
#
# - Python: 3.9 or higher
# - RAM: 16GB minimum (32GB recommended for training)
# - GPU: CUDA 11.8+ (optional, for faster training)
# - Storage: 10GB for models and datasets
# - OS: Windows 10+, Linux, macOS 10.15+
#
# ====================================================================
# ESTIMATED INSTALLATION SIZE: ~8GB
# ESTIMATED INSTALLATION TIME: 10-15 minutes
# ====================================================================
